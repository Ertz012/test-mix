import pandas as pd
import numpy as np
from scipy.stats import expon

def run_traffic_analysis(file_ingress, file_egress, target_src_id, mix_delay_mu):
    """
    Führt die Traffic Analysis basierend auf Danezis' Paper durch.
    
    Args:
        file_ingress: Pfad zur CSV mit eingehenden Nachrichten.
        file_egress: Pfad zur CSV mit ausgehenden Nachrichten.
        target_src_id: Die 'src' ID, die wir verfolgen wollen (das Opfer).
        mix_delay_mu: Der Parameter mu der Exponentialverteilung (Rate). 
                      ACHTUNG: mu = 1 / durchschnittliche_verzögerung.
                      Für avg_delay=2s muss mix_delay_mu=0.5 sein.
    """
    
    # 1. Daten laden
    print(f"Lade Daten...")
    try:
        df_in = pd.read_csv(file_ingress)
        df_out = pd.read_csv(file_egress)
    except FileNotFoundError as e:
        print(f"Fehler: Datei nicht gefunden - {e}")
        return

    # Sortieren nach Zeit
    df_in = df_in.sort_values('timestamp')
    df_out = df_out.sort_values('timestamp')

    # Globale Zeitgrenzen bestimmen
    if df_in.empty or df_out.empty:
        print("Fehler: Eine der Dateien ist leer.")
        return

    t_start = min(df_in['timestamp'].min(), df_out['timestamp'].min())
    t_end = max(df_in['timestamp'].max(), df_out['timestamp'].max())
    duration = t_end - t_start
    
    # Normalisieren der Zeitstempel auf t=0
    df_in['t_norm'] = df_in['timestamp'] - t_start
    df_out['t_norm'] = df_out['timestamp'] - t_start

    print(f"Verwende festes mu (Rate): {mix_delay_mu}")
    print(f"  -> Entspricht einer durchschnittlichen Verzögerung von: {1.0/mix_delay_mu:.2f}s")

    # 2. Das Eingangssignal f(t) des Ziels extrahieren
    target_traffic = df_in[df_in['src'] == target_src_id]
    if target_traffic.empty:
        print(f"Keine Nachrichten für src {target_src_id} gefunden.")
        return

    print(f"Analysiere Ziel '{target_src_id}' mit {len(target_traffic)} Nachrichten.")

    # 3. Diskretisierung und Faltung
    # Parameter für die Auflösung
    bin_size = 0.1 # 100ms
    num_bins = int(np.ceil(duration / bin_size)) + 1
    
    # Histogramm des Inputs (f(t))
    input_counts, _ = np.histogram(target_traffic['t_norm'], bins=num_bins, range=(0, duration))
    input_signal = input_counts / bin_size # Dichte

    # Delay Charakteristik d(x) = mu * e^(-mu * x)
    # Paper Referenz: Gleichung (10) [cite: 73]
    # Wir berechnen den Kernel bis die Wahrscheinlichkeit vernachlässigbar ist
    max_delay_influence = expon.ppf(0.999, scale=1/mix_delay_mu) 
    kernel_bins = int(np.ceil(max_delay_influence / bin_size))
    kernel_x = np.linspace(0, max_delay_influence, kernel_bins)
    
    # scipy.stats.expon verwendet scale=1/lambda (also scale=1/mu)
    delay_kernel = expon.pdf(kernel_x, scale=1/mix_delay_mu)
    delay_kernel /= delay_kernel.sum() # Normalisieren für diskrete Faltung

    # Faltung: (d * f)(t) - Referenz Gleichung (24) [cite: 180]
    convolved_signal = np.convolve(input_signal, delay_kernel, mode='full')[:num_bins]
    
    # 4. Likelihood Berechnung für jeden möglichen Ausgang
    results = []
    unique_dsts = df_out['dst'].unique()
    
    lambda_f = len(target_traffic) / duration # Rate des Ziels
    u_prob = 1.0 / duration # Gleichverteilung (Noise Modell)

    print(f"Berechne Likelihoods für {len(unique_dsts)} mögliche Ziele...")

    for dst in unique_dsts:
        dst_traffic = df_out[df_out['dst'] == dst]
        if len(dst_traffic) == 0:
            continue
            
        observations = dst_traffic['t_norm'].values
        
        # Mapping auf Bins
        bin_indices = (observations / bin_size).astype(int)
        bin_indices = np.clip(bin_indices, 0, num_bins - 1)
        
        lambda_x = len(dst_traffic) / duration
        
        # Signalwerte an den Beobachtungszeitpunkten
        signal_vals = convolved_signal[bin_indices]
        
        # Berechnung Wahrscheinlichkeitsdichte C_X(t) - Referenz Gleichung (23) [cite: 176]
        # C_X(t) = (lambda_f * (d*f)(t) + (lambda_x - lambda_f) * U) / lambda_x
        term_signal = lambda_f * signal_vals
        term_noise = (lambda_x - lambda_f) * u_prob
        term_noise = np.maximum(term_noise, 0) # Rauschen kann nicht negativ sein
        
        c_x_vals = (term_signal + term_noise) / lambda_x
        
        # Numerische Stabilität für log
        c_x_vals = np.maximum(c_x_vals, 1e-20)
        
        # Log-Likelihood Summe - Referenz Gleichung (28) [cite: 253]
        score = np.sum(np.log(c_x_vals))
        
        results.append({
            'dst': dst,
            'score': score,
            'packet_count': len(dst_traffic)
        })

    # 5. Ergebnisse ausgeben
    results_df = pd.DataFrame(results)
    if not results_df.empty:
        results_df = results_df.sort_values('score', ascending=False).reset_index(drop=True)
    return results_df

# --- Beispielaufruf ---
if __name__ == "__main__":
    # Konfiguration
    MU = 0.5  # Rate (entspricht 2s Mean Delay)
    TARGET = 'c1'
    IN_FILE = 'D:/Uni Hamburg/Module/MASTER/test-mix/logs/Testrun_20260117_122957_06_backup_mixes_long/system_in.csv'
    OUT_FILE = 'D:/Uni Hamburg/Module/MASTER/test-mix/logs/Testrun_20260117_122957_06_backup_mixes_long/system_out.csv'
    
    # Analyse starten
    ranking = run_traffic_analysis(IN_FILE, OUT_FILE, TARGET, MU)
    
    if ranking is not None and not ranking.empty:
        print("\n--- Top 5 Verdächtige ---")
        print(ranking.head(5))
    else:
        print("Keine Ergebnisse.")